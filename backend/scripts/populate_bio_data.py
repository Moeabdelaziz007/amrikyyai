"""
Script to populate Amrikyy AI database with personal bio data
"""

import asyncio
import json
from datetime import datetime
from sqlalchemy.orm import Session

from app.core.database import SessionLocal, Base, engine
from app.models.database import Document, DocumentChunk
from app.services.embedding_service import EmbeddingService
from app.services.vector_service import VectorService

# Bio data for Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ² (Amrikyy)
BIO_DATA = [
    {
        "id": "bio-1",
        "title": "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©",
        "text": "Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ² (Amrikyy)ØŒ Ø·Ø§Ù„Ø¨ Ø¯Ø±Ø§Ø³Ø§Øª Ø¹Ù„ÙŠØ§ ÙÙŠ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§. Ù…ÙˆØ§Ø·Ù† Ø£Ù…Ø±ÙŠÙƒÙŠ ÙˆÙ…ØµØ±ÙŠØŒ Ù…ÙˆÙ„ÙˆØ¯ ÙÙŠ 10 ÙŠÙˆÙ„ÙŠÙˆ 1999 ÙÙŠ Ù…ØµØ±.",
        "metadata": {"category": "personal", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-2", 
        "title": "Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ù‡Ù†ÙŠ",
        "text": "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ØªØ®ØµØµØ§Øª Ø¨Ø®Ø¨Ø±Ø© Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Web3ØŒ UXØŒ ÙˆØ§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù…Ø¹ØªÙ…Ø¯ Ù…Ù† OpenAIØŒ IntelØŒ ÙˆL'OrÃ©al. Ù…Ø§Ù‡Ø± ÙÙŠ PythonØŒ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØŒ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØŒ ÙˆØ³Ø±Ø¯ Ø§Ù„Ù‚ØµØµ Ø§Ù„Ø±Ù‚Ù…ÙŠØ©. Ø´ØºÙˆÙ Ø¨Ø¨Ù†Ø§Ø¡ Ø­Ù„ÙˆÙ„ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© ØªØ±Ø¨Ø· Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠ.",
        "metadata": {"category": "summary", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-3",
        "title": "Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª", 
        "text": "Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØŒ ØªØµÙ…ÙŠÙ… UX/UIØŒ SEOØŒ Ù†Ù…Ø°Ø¬Ø© Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø«Ù‚Ø§ÙØ§ØªØŒ A/B TestingØŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„ØªØµÙ…ÙŠÙ…ÙŠØŒ Python Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø£ØªÙ…ØªØ©ØŒ Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø¨Ù„ÙˆÙƒØ´ÙŠÙ†.",
        "metadata": {"category": "skills", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-4",
        "title": "Ø§Ù„Ø®Ø¨Ø±Ø© â€” Innovation & Strategy Intern",
        "text": "Global Career Accelerator (Ø¹Ù† Ø¨ÙØ¹Ø¯) â€” Ù…Ø§ÙŠÙˆ 2025 Ø­ØªÙ‰ Ø£ØºØ³Ø·Ø³ 2025. Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ù…Ø­ØªÙˆÙ‰ ÙˆØªØ¬Ø§Ø±Ø¨ Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ù…Ø¹ L'OrÃ©al ÙˆGRAMMY U ÙˆIntel ÙˆUNESCOØŒ Ø´Ù…Ù„Øª Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª A/BØŒ ØªØ­Ø³ÙŠÙ†Ø§Øª UXØŒ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©ØŒ ØªØ·ÙˆÙŠØ± ØµÙØ­Ø§Øª Ù‡Ø¨ÙˆØ·ØŒ ÙˆØ¨Ù†Ø§Ø¡ Ø´Ø®ØµÙŠØ§Øª Ù…Ø³ØªØ®Ø¯Ù….",
        "metadata": {"category": "experience", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-5",
        "title": "Ø§Ù„Ø®Ø¨Ø±Ø© â€” Freelance Projects", 
        "text": "Ù…Ù† Ù…Ø§ÙŠÙˆ 2023 Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†: Ø¨Ù†Ø§Ø¡ Ø£Ø¯ÙˆØ§Øª ÙˆÙ„ÙˆØ­Ø§Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØªØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠØ© ÙˆWeb3 ÙˆØªØµÙ…ÙŠÙ… UX. Ø£Ù…Ø«Ù„Ø©: Moe QuantumAI DashboardØŒ Ù…Ø´Ø±ÙˆØ¹ StayX ÙÙŠ ØªØ­Ø¯ÙŠ Coinbase Web3ØŒ ÙˆØ£Ø¯ÙˆØ§Øª ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± AI. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ¬Ù‡ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ÙˆØªØ¯ÙÙ‚Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©.",
        "metadata": {"category": "experience", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-6",
        "title": "Ø§Ù„Ø®Ø¨Ø±Ø© â€” Crypto Derivatives Trader",
        "text": "Bybit â€” Ø¹Ù† Ø¨ÙØ¹Ø¯ â€” Ù…Ù† ÙŠÙ†Ø§ÙŠØ± 2020 Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†: ØªÙ†ÙÙŠØ° ØªØ¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª ÙˆØ§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© ÙÙŠ Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø´ÙØ±Ø©ØŒ ØªØ­Ù„ÙŠÙ„ Ø­Ø±ÙƒØ© Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±ØŒ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ¯Ø§ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ù„Ø­Ø¸ÙŠØ©.",
        "metadata": {"category": "experience", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-7",
        "title": "Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
        "text": "Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³ Ø¹Ù„ÙˆÙ… ÙÙŠ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ â€” Ø¬Ø§Ù…Ø¹Ø© Kennesaw State (2022â€“Ø§Ù„Ø­Ø§Ø¶Ø±). Ø¯Ø¨Ù„ÙˆÙ… Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ â€” Chattahoochee Technical College (2017â€“2021).",
        "metadata": {"category": "education", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-8", 
        "title": "Ø§Ù„Ø´Ù‡Ø§Ø¯Ø§Øª",
        "text": "Ø´Ù‡Ø§Ø¯Ø© UX/UI & Prototyping Ù…Ù† L'OrÃ©al Ã— GCA (Ø£ØºØ³Ø·Ø³ 2025). Ø´Ù‡Ø§Ø¯Ø© AI Professional Skills Ù…Ù† OpenAI Ã— GCA (Ø£ØºØ³Ø·Ø³ 2025). Ø´Ù‡Ø§Ø¯Ø© Understanding LLMs and Basic Prompting Techniques Ù…Ù† CodeSignal (Ø£ØºØ³Ø·Ø³ 2025). Ø´Ù‡Ø§Ø¯Ø© Intercultural Skills Ù…Ù† UNESCO Ã— GCA (Ø£ØºØ³Ø·Ø³ 2025). Ø´Ù‡Ø§Ø¯Ø© Frontend Developer Ù…Ù† HackerRank (ÙŠÙˆÙ„ÙŠÙˆ 2025). Ø´Ù‡Ø§Ø¯Ø© Data Visualization Ù…Ù† Intel Ã— GCA (ÙŠÙˆÙ†ÙŠÙˆ 2025).",
        "metadata": {"category": "certifications", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-9",
        "title": "Ø§Ù„Ø¬ÙˆØ§Ø¦Ø² ÙˆØ§Ù„ØªÙƒØ±ÙŠÙ…", 
        "text": "Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…ÙŠØ¯ â€” Ø¬Ø§Ù…Ø¹Ø© Kennesaw State (ÙŠÙˆÙ†ÙŠÙˆ 2024 ÙˆØ¯ÙŠØ³Ù…Ø¨Ø± 2023) Ù„ØªÙ…ÙŠØ² Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙˆØ§Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ù„ Ù…Ø±ØªÙØ¹ ÙÙŠ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ.",
        "metadata": {"category": "awards", "source": "LinkedIn", "lang": "ar"}
    },
    {
        "id": "bio-10",
        "title": "Ø§Ù„Ù„ØºØ§Øª",
        "text": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù„Ù‡Ø¬Ø© Ù…ØµØ±ÙŠØ©) â€” Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£Ù…. Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© â€” Ù…Ø³ØªÙˆÙ‰ Ù…ØªÙ‚Ø¯Ù….",
        "metadata": {"category": "languages", "source": "LinkedIn", "lang": "ar"}
    }
]

async def create_bio_document():
    """Create a consolidated bio document"""
    
    # Combine all bio sections into one document
    full_bio = "# Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© - Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ² (Amrikyy)\n\n"
    
    for section in BIO_DATA:
        full_bio += f"## {section['title']}\n"
        full_bio += f"{section['text']}\n\n"
    
    db = SessionLocal()
    try:
        # Create document record
        document = Document(
            filename="amrikyy_bio.md",
            original_filename="amrikyy_bio.md", 
            content_type="text/markdown",
            size=len(full_bio.encode('utf-8')),
            file_path="/virtual/bio/amrikyy_bio.md",
            status="completed",
            title="Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© - Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ² (Amrikyy)",
            author="Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ²",
            language="ar",
            processed_at=datetime.utcnow()
        )
        
        db.add(document)
        db.commit()
        db.refresh(document)
        
        print(f"âœ… Created document: {document.id}")
        
        # Create chunks for each bio section
        embedding_service = EmbeddingService()
        vector_service = VectorService()
        
        for i, section in enumerate(BIO_DATA):
            # Create document chunk
            chunk_content = f"{section['title']}\n{section['text']}"
            
            chunk = DocumentChunk(
                document_id=document.id,
                content=chunk_content,
                chunk_index=i,
                section_title=section['title'],
                content_hash=hash(chunk_content)
            )
            
            db.add(chunk)
            db.commit()
            db.refresh(chunk)
            
            # Generate embedding
            try:
                embedding = await embedding_service.embed_text(chunk_content)
                
                # Store in vector database
                vector_id = await vector_service.upsert_vector(
                    vector_id=str(chunk.id),
                    embedding=embedding,
                    metadata={
                        "document_id": str(document.id),
                        "chunk_id": str(chunk.id),
                        "title": section['title'],
                        "category": section['metadata']['category'],
                        "source": section['metadata']['source'],
                        "language": section['metadata']['lang'],
                        "content": chunk_content[:500]  # First 500 chars for preview
                    }
                )
                
                # Update chunk with embedding ID
                chunk.embedding_id = vector_id
                db.commit()
                
                print(f"âœ… Created chunk {i+1}/10: {section['title']}")
                
            except Exception as e:
                print(f"âŒ Failed to create embedding for chunk {i+1}: {e}")
                continue
        
        print(f"ğŸ‰ Successfully populated bio data for Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ² (Amrikyy)")
        print(f"ğŸ“„ Document ID: {document.id}")
        print(f"ğŸ“¦ Total chunks: {len(BIO_DATA)}")
        
        return document.id
        
    except Exception as e:
        print(f"âŒ Error creating bio document: {e}")
        db.rollback()
        raise
    finally:
        db.close()

async def test_bio_retrieval():
    """Test retrieval of bio information"""
    from app.services.retrieval_service import RetrievalService
    
    retrieval_service = RetrievalService()
    
    # Test queries
    test_queries = [
        "Ù…Ù† Ù‡Ùˆ Ù…Ø­Ù…Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ø¹Ø²ÙŠØ²ØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ø®Ø¨Ø±Ø§Øª Amrikyy Ø§Ù„Ù…Ù‡Ù†ÙŠØ©ØŸ", 
        "Ø£ÙŠÙ† Ø¯Ø±Ø³ Ù…Ø­Ù…Ø¯ØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ù…Ù‡Ø§Ø±Ø§Øª Amrikyy Ø§Ù„ØªÙ‚Ù†ÙŠØ©ØŸ",
        "What is Amrikyy's background in AI?",
        "Tell me about Mohamed's certifications"
    ]
    
    print("\nğŸ” Testing bio retrieval...")
    
    for query in test_queries:
        try:
            results = await retrieval_service.retrieve_by_text(
                query_text=query,
                top_k=3
            )
            
            print(f"\nâ“ Query: {query}")
            print(f"ğŸ“Š Found {len(results)} results:")
            
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.get('title', 'Unknown')}")
                print(f"     Score: {result.get('confidence', 0):.3f}")
                print(f"     Preview: {result.get('content', '')[:100]}...")
                
        except Exception as e:
            print(f"âŒ Query failed: {query} - {e}")

if __name__ == "__main__":
    print("ğŸš€ Populating Amrikyy bio data...")
    
    # Create tables if they don't exist
    Base.metadata.create_all(bind=engine)
    
    # Run the population
    asyncio.run(create_bio_document())
    
    # Test retrieval
    asyncio.run(test_bio_retrieval())
    
    print("\nâœ… Bio data population complete!")
